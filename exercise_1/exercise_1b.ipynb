{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 1b\n",
    "\n",
    "\n",
    "\n",
    "### Understand Computation Graphs and Backpropagation\n",
    "\n",
    "- Compute the gradients along the given network manually.\n",
    "- Compare your results to the gradients computed by pytorch's autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 2, bias=False),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(2, 1, bias=False),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "simple_net.load_state_dict(torch.load('initial_model.pth'))\n",
    "\n",
    "\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = torch.Tensor([[-2, -0.3, 1.7, 0.2]])\n",
    "target = torch.Tensor([[1.]])\n",
    "\n",
    "prediction = simple_net(in_features)\n",
    "\n",
    "loss = criterion(prediction, target) \n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5130, -0.0770,  0.4361,  0.0513],\n",
       "        [ 0.4947,  0.0742, -0.4205, -0.0495]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_net[0].weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0709, -0.2826]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_net[2].weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 Matrix :Parameter containing:\n",
      "tensor([[ 0.1433, -0.2529,  0.2200,  0.0073],\n",
      "        [-0.3766,  0.0989, -0.0612,  0.1814]], requires_grad=True)\n",
      "\n",
      "W2 Matrix :Parameter containing:\n",
      "tensor([[-0.5954,  0.5741]], requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Pretrained weights of the model layer 1 and layer 2\n",
    "w1= simple_net[0].weight\n",
    "w2=simple_net[2].weight\n",
    "print (\"W1 Matrix :{}\\n\".format(w1))\n",
    "print (\"W2 Matrix :{}\\n\".format(w2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'first_oper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2ca61bdf6c48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#First operation  = wX (input multiplied by the weight matrix)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfirst_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Output of first hidden layer: \\n{}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_oper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'first_oper' is not defined"
     ]
    }
   ],
   "source": [
    "#First operation  = wX (input multiplied by the weight matrix)\n",
    "first_out = np.dot(w1.detach().cpu(), in_features.T)\n",
    "print (\"Output of first hidden layer: \\n{}\\n\".format(first_oper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relu applied on first layer output\n",
    "relu_out_first = relu = F.relu(torch.tensor(first_out))\n",
    "print (\"Output after RELU: \\n {}\\n\".format(relu_out_first))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of first layer multiplied by weight matrix of second layer\n",
    "second_out = np.dot(w2.detach().cpu(), relu_out_first)\n",
    "print (\"Output of output layer: \\n{}\\n\".format(second_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relu applied on first layer output\n",
    "sigmoid_out = F.sigmoid(torch.tensor(second_out)).detach().numpy()\n",
    "print (\"Output after Sigmoid: \\n {}\\n\".format(sigmoid_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_loss =  - np.log(sigmoid_out) # -Y(log(y)) for case where y = 1 in binary cross entropy\n",
    "print(forward_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACK PROPAGATION\n",
    "\n",
    "#### derivative of BCE when y = 1:   $  -1/x $\n",
    "#### derivative of sigmoid:   $ e^-x / (1 - e^-x)^-2 $\n",
    "#### derivative of ReLU:         1 when x>0 else 0  \n",
    "#### derivative of (wX) w.r.t `X` is `w` and w.r.t `w` is `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative of loss w.r.t loss is 1\n",
    "grad_loss = 1\n",
    "\n",
    "#derivative of loss w.r.t sigmoid_out is `derivative of loss w.r.t loss * derivative of loss w.r.t sigmoid_out` chain rule\n",
    "grad_loss_wrt_sig = -(1/sigmoid_out) * grad_loss\n",
    "print (\"Gradient of loss W.R.T output of sigmoid :\",grad_loss_wrt_sig.item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative of sigmoid W.R.T input\n",
    "grad_sigmoid_wrt_w2_2_relu = (np.exp(-second_out) / (1 + np.exp(-second_out))**(2)) * grad_loss_wrt_sig\n",
    "print (\"Gradient of sigmoid activation w.r.t input :\", grad_sigmoid_wrt_w2_2_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients of second layer w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative of wX w.r.t weights. here w is weights of second layer and X is output of ReLU\n",
    "grad_wX_wrt_w_2 =  relu_out_first * grad_sigmoid_wrt_w2_2_relu\n",
    "print (\"Gradiends of second layer weights:\", grad_wX_wrt_w_2.reshape(-1))\n",
    "print (\"Pytorch gradients :\", simple_net[2].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative of wX w.r.t `X`. here w is weights of second layer and X is output of ReLU\n",
    "grad_wX_wrt_X2 =  w2.detach().numpy() * grad_sigmoid_wrt_w2_2_relu\n",
    "print (\"Gradiends of wX w.r.t X2:\", grad_wX_wrt_X2.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient of Relu is \n",
    "# 1 for x > 0; \n",
    "# 0 for x <= 0\n",
    "grad_relu = 1 * grad_wX_wrt_X2 # since input to relu is > 0 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients First layer W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_w1 = in_features * grad_relu.reshape(2,1)\n",
    "print (\"Manually computed gradients W1: \\n\", grad_w1)\n",
    "print (\"\\nPytorch gradients W1: \\n\", simple_net[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "##### Gradients computed from pytorch backprop and manual backprop are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
